{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('/Circuit_LoRa'))\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" \n",
    "\n",
    "from changecircuit_edge_keylayer import define_critical_layers_via_edges\n",
    "from circuit_weighted_lora import (\n",
    "    apply_circuit_weighted_lora,\n",
    "    freeze_non_critical_layers,\n",
    "    circuit_regularization,\n",
    "    save_initial_params\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_critical_layers(before_circuit_json_path, after_circuit_json_path, model_prefix='base_model.model.gpt_neox.layers', threshold=None, top_k=5):\n",
    "\n",
    "    if top_k == 0:  \n",
    "        return []\n",
    "    critical_layers = define_critical_layers_via_edges(\n",
    "        before_circuit_json_path,\n",
    "        after_circuit_json_path,\n",
    "        model_prefix=model_prefix,\n",
    "        threshold=threshold,\n",
    "        top_k=top_k\n",
    "    )\n",
    "    \n",
    "    for layer in critical_layers:\n",
    "        print(layer)\n",
    "    \n",
    "    return critical_layers\n",
    "\n",
    "before_circuit_json_path = '/2_arithmetic_operations_100/graph_results_100/lora_graph_results/graph_100_2arithmetic_operations_1.4b_epoch_0.json'  # 微调前的电路JSON路径\n",
    "after_circuit_json_path = '/2_arithmetic_operations_100/graph_results_100/lora_graph_results/graph_100_2arithmetic_operations_1.4b_r2_epoch_2.json'    # 微调后的电路JSON路径\n",
    "\n",
    "model_prefix = 'gpt_neox.layers'\n",
    "\n",
    "threshold = None  \n",
    "top_k = 0       \n",
    "\n",
    "critical_layers = identify_critical_layers(\n",
    "    before_circuit_json_path,\n",
    "    after_circuit_json_path,\n",
    "    model_prefix=model_prefix,\n",
    "    threshold=threshold,\n",
    "    top_k=top_k\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(train_file, validation_file, tokenizer):\n",
    "   \n",
    "    data_files = {\n",
    "        'train': train_file,\n",
    "        'validation': validation_file\n",
    "    }\n",
    "    dataset = load_dataset('json', data_files=data_files)\n",
    "    \n",
    "    def preprocess_function(examples):\n",
    "        max_length = 32\n",
    "        inputs = examples['input']\n",
    "        outputs = [str(o) for o in examples['output']]\n",
    "\n",
    "        prompts = [f\"{inp}\\n\" for inp in inputs]\n",
    "        full_texts = [prompt + out for prompt, out in zip(prompts, outputs)]\n",
    "\n",
    "        tokenized_full = tokenizer(full_texts, truncation=True, padding='max_length', max_length=max_length)\n",
    "\n",
    "        tokenized_prompt = tokenizer(prompts, truncation=True, padding='max_length', max_length=max_length)\n",
    "\n",
    "        labels = []\n",
    "        for i in range(len(full_texts)):\n",
    "            prompt_len = len(tokenizer.encode(prompts[i], truncation=True, max_length=max_length))\n",
    "\n",
    "            label = [-100] * prompt_len + tokenized_full['input_ids'][i][prompt_len:]\n",
    "            label = label[:max_length]\n",
    "            if len(label) < max_length:\n",
    "                label += [-100] * (max_length - len(label))\n",
    "            labels.append(label)\n",
    "\n",
    "        tokenized_full['labels'] = labels\n",
    "\n",
    "        return tokenized_full\n",
    "    \n",
    "    tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "    \n",
    "    return tokenized_datasets\n",
    "\n",
    "train_file = '/2_arithmetic_operations_100/finetune_pythia_100/finetune_data/train_100.jsonl'\n",
    "validation_file = '/2_arithmetic_operations_100/finetune_pythia_100/finetune_data/test_100.jsonl'\n",
    "\n",
    "model_name = 'EleutherAI/gpt-neo-2.7B'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenized_datasets = load_and_preprocess_data(train_file, validation_file, tokenizer)\n",
    "\n",
    "print(tokenized_datasets['train'][:5])\n",
    "print(tokenized_datasets['validation'][:5])\n",
    "\n",
    "train_size = len(tokenized_datasets['train'])\n",
    "validation_size = len(tokenized_datasets['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model = apply_circuit_weighted_lora(\n",
    "    model=model,\n",
    "    critical_layers=critical_layers,\n",
    "    r=32,                 \n",
    "    alpha=64,             \n",
    "    extra_r=0,             \n",
    "    critical_alpha=0,    \n",
    "    dropout=0           \n",
    ")\n",
    "\n",
    "model = freeze_non_critical_layers(model, critical_layers)\n",
    "\n",
    "initial_params = save_initial_params(model, critical_layers)\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    total_params = 0\n",
    "    print(\"\\nTrainable Parameters:\")\n",
    "    for name, param in model.named_parameters():\n",
    "        total_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "            print(f\" - {name}: {param.numel()} parameters\")\n",
    "    print(f\"\\nTotal trainable params: {trainable_params} / Total params: {total_params}\")\n",
    "\n",
    "print_trainable_parameters(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from safetensors.torch import save_file\n",
    "\n",
    "def save_updated_weights(model, critical_layers, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    updated_weights = {}\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"lora_\" in name and param.requires_grad:\n",
    "            updated_weights[name] = param.detach().cpu()\n",
    "            print(f\"Saving LoRA weight: {name}\")\n",
    "        elif any(layer in name for layer in critical_layers) and param.requires_grad:\n",
    "            updated_weights[name] = param.detach().cpu()\n",
    "            print(f\"Saving critical layer weight: {name}\")\n",
    "\n",
    "    weights_path = os.path.join(output_dir, \"adapter_model.safetensors\")\n",
    "    try:\n",
    "        save_file(updated_weights, weights_path)\n",
    "        print(f\"Updated weights saved to {weights_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving safetensors file: {e}\")\n",
    "\n",
    "    config_path = os.path.join(output_dir, \"adapter_config.json\")\n",
    "    with open(config_path, \"w\") as f:\n",
    "        config = {\n",
    "            \"critical_layers\": critical_layers,\n",
    "            \"r\": 32,\n",
    "            \"alpha\": 64,\n",
    "            \"extra_r\": 0,\n",
    "            \"critical_alpha\": 0,\n",
    "            \"dropout\": 0,\n",
    "        }\n",
    "        json.dump(config, f, indent=4)\n",
    "    print(f\"LoRA config saved to {config_path}\")\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, critical_layers=None, initial_params=None, lambda_reg=1e-3, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.critical_layers = critical_layers  \n",
    "        self.initial_params = initial_params    \n",
    "        self.lambda_reg = lambda_reg           \n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        if self.critical_layers and self.initial_params:\n",
    "            reg_loss = circuit_regularization(model, self.critical_layers, self.initial_params, self.lambda_reg)\n",
    "            loss = loss + reg_loss\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def save_model(self, output_dir=None, **kwargs):\n",
    "        if output_dir is None:\n",
    "            output_dir = self.args.output_dir\n",
    "        save_updated_weights(self.model, self.critical_layers, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./lora_gpt_results/r32a64',  # Output directory\n",
    "    num_train_epochs=2,                            # Number of training epochs\n",
    "    per_device_train_batch_size=8,                 # Batch size per device\n",
    "    warmup_steps=25,                               # Number of warmup steps\n",
    "    weight_decay=0.01,                             # Weight decay\n",
    "    logging_dir='./circuit_weighted_lora_logs',    # Logging directory\n",
    "    logging_steps=10,                              # Log every 10 steps\n",
    "    save_steps=25,                                 # Save model every 50 steps\n",
    "    save_strategy=\"steps\",                         # Save by steps\n",
    "    save_total_limit=10,                            # Keep at most 1 model\n",
    "    fp16=True,                                     # Mixed precision\n",
    "    gradient_accumulation_steps=4,                 # Gradient accumulation steps\n",
    "    report_to=\"none\",                              # Disable default reporting\n",
    "    learning_rate=2e-4,                            # Learning rate (higher than for full fine-tuning)\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer_grouped_parameters = []\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        if any(layer in name for layer in critical_layers):\n",
    "            lr = 3e-4  \n",
    "            print(f\"Critical Layer Param: {name} | Learning Rate: {lr}\")\n",
    "        else:\n",
    "            lr = 3e-4 \n",
    "            print(f\"Non-Critical Layer Param: {name} | Learning Rate: {lr}\")\n",
    "        optimizer_grouped_parameters.append({\"params\": param, \"lr\": lr})\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, weight_decay=0.01)\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    critical_layers=critical_layers,\n",
    "    initial_params=initial_params,\n",
    "    lambda_reg=0,  \n",
    "    optimizers=(optimizer, None)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    critical_layers=critical_layers,\n",
    "    initial_params=initial_params,\n",
    "    lambda_reg=0,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
