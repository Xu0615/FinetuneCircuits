{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the Add/Sub Circuit Using EAP(-IG)\n",
    "\n",
    "First, we import various packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from functools import partial\n",
    "from typing import Optional, List, Union, Literal, Tuple\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import PreTrainedTokenizer\n",
    "from transformer_lens import HookedTransformer\n",
    "import transformer_lens.utils as utils\n",
    "from transformers import AutoTokenizer \n",
    "from eap.graph import Graph\n",
    "from eap.evaluate import evaluate_graph, evaluate_baseline,get_circuit_logits\n",
    "from eap.attribute import attribute \n",
    "from eap.attribute import tokenize_plus\n",
    "from eap.metrics import logit_diff, direct_logit\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Metrics\n",
    "\n",
    "This package expects data to come from a dataloader. Each item consists of clean and corrupted paired inputs (strings), as well as a label (encoded as a token id). For convenience, we've included a dataset in that form as a CSV (more to come with the full code of the paper).\n",
    "\n",
    "A metric takes in the model's (possibly corrupted) logits, clean logits, input lengths, and labels. It computes a metric value for each batch item; this can either be used as is, or turned into a loss (lower is better), or meaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_EAP(xs):\n",
    "    clean, corrupted, labels = zip(*xs)\n",
    "    clean = list(clean)\n",
    "    corrupted = list(corrupted)\n",
    "    return clean, corrupted, labels\n",
    "\n",
    "class EAPDataset(Dataset):\n",
    "    def __init__(self, filepath):\n",
    "        self.df = pd.read_csv(filepath)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def shuffle(self):\n",
    "        self.df = self.df.sample(frac=1)\n",
    "\n",
    "    def head(self, n: int):\n",
    "        self.df = self.df.head(n)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        return row['clean'], row['corrupted'], row['label']\n",
    "    \n",
    "    def to_dataloader(self, batch_size: int):\n",
    "        return DataLoader(self, batch_size=batch_size, collate_fn=collate_EAP)\n",
    "    \n",
    "def get_logit_positions(logits: torch.Tensor, input_length: torch.Tensor):\n",
    "    batch_size = logits.size(0)\n",
    "    idx = torch.arange(batch_size, device=logits.device)\n",
    "\n",
    "    logits = logits[idx, input_length - 1]\n",
    "    return logits\n",
    "\n",
    "\n",
    "def kl_divergence(logits: torch.Tensor, clean_logits: torch.Tensor, input_length: torch.Tensor, labels: torch.Tensor, mean=True, loss=True):\n",
    "    logits = get_logit_positions(logits, input_length)\n",
    "    clean_logits = get_logit_positions(clean_logits, input_length)\n",
    "\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    clean_probs = torch.softmax(clean_logits, dim=-1)\n",
    "\n",
    "    results = F.kl_div(probs.log(), clean_probs.log(), log_target=True, reduction='none').mean(-1)\n",
    "    return results.mean() if mean else results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing EAP-IG\n",
    "\n",
    "First, we load the model, data, and metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'pythia-1.4b-deduped'\n",
    "\n",
    "from safetensors.torch import load_file\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "base_model_path = \"EleutherAI/pythia-1.4b-deduped\"\n",
    "lora_weights_path = \"/add_sub_mul_div/finetune_pythia_steps/PEFT/add_sub/lora_results/r32a64/checkpoint-500/adapter_model.safetensors\"\n",
    "\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_path)\n",
    "lora_weights = load_file(lora_weights_path)\n",
    "\n",
    "scaling = 2  \n",
    "scaling_extra = 2  \n",
    "\n",
    "\n",
    "def merge_lora_weights(model, lora_weights, scaling=2.0, scaling_extra=2.0):\n",
    "    for name, param in model.named_parameters():\n",
    "        layer_name = name.rsplit(\".\", 1)[0]  \n",
    "\n",
    "        if \"bias\" in name:\n",
    "            if name in lora_weights and lora_weights[name].shape == param.data.shape:\n",
    "                param.data += lora_weights[name].to(param.device)\n",
    "                print(f\"Applied LoRA bias to: {name}\")\n",
    "            else:\n",
    "                print(f\"Skipped LoRA bias update: {name}\")\n",
    "            continue\n",
    "\n",
    "        lora_A_key = f\"{layer_name}.lora_A\"\n",
    "        lora_B_key = f\"{layer_name}.lora_B\"\n",
    "        lora_A_extra_key = f\"{layer_name}.lora_A_extra\"\n",
    "        lora_B_extra_key = f\"{layer_name}.lora_B_extra\"\n",
    "\n",
    "        delta_weight = None\n",
    "\n",
    "        if lora_A_key in lora_weights and lora_B_key in lora_weights:\n",
    "            lora_A = lora_weights[lora_A_key].to(param.device)\n",
    "            lora_B = lora_weights[lora_B_key].to(param.device)\n",
    "            delta_weight = torch.matmul(lora_B, lora_A) * scaling  \n",
    "            print(f\"Applied standard LoRA to: {layer_name}\")\n",
    "\n",
    "        if lora_A_extra_key in lora_weights and lora_B_extra_key in lora_weights:\n",
    "            lora_A_extra = lora_weights[lora_A_extra_key].to(param.device)\n",
    "            lora_B_extra = lora_weights[lora_B_extra_key].to(param.device)\n",
    "            extra_delta = torch.matmul(lora_B_extra, lora_A_extra) * scaling_extra \n",
    "            delta_weight = delta_weight + extra_delta if delta_weight is not None else extra_delta\n",
    "            print(f\"Applied extra LoRA to: {layer_name}\")\n",
    "\n",
    "        if delta_weight is not None and delta_weight.shape == param.data.shape:\n",
    "            param.data += delta_weight\n",
    "            print(f\"Updated weight: {name}\")\n",
    "        elif delta_weight is not None:\n",
    "            print(f\"Shape mismatch: {layer_name} - Î”W {delta_weight.shape}, param {param.shape}\")\n",
    "        else:\n",
    "            print(f\"No LoRA update for: {name}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "hf_model = merge_lora_weights(base_model, lora_weights)\n",
    "\n",
    "model = HookedTransformer.from_pretrained(model_name, device='cuda',hf_model=hf_model)\n",
    "model.cfg.use_split_qkv_input = True\n",
    "model.cfg.use_attn_result = True\n",
    "model.cfg.use_hook_mlp_in = True\n",
    "\n",
    "tokenizer = model.tokenizer\n",
    "ds = EAPDataset('/add_sub_mul_div/4_arithmetic_data/add_sub/add_sub_circuit.csv')\n",
    "dataloader = ds.to_dataloader(64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we perform EAP! We instantiate an unscored graph from the model, and use the attribute method to score it. This requires a model, graph, dataloader, and loss. We set `method='EAP-IG'`, and set the number of iterations via `ig_steps`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a graph with a model\n",
    "g = Graph.from_model(model)\n",
    "\n",
    "# Attribute using the model, graph, clean / corrupted data and labels, as well as a metric\n",
    "attribute(model, g, dataloader, partial(kl_divergence, loss=True, mean=True), method='EAP-IG', ig_steps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.count_included_nodes(), g.count_included_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print total number of edges in the graph\n",
    "total_edges = len(g.edges)\n",
    "print(f\"Total number of edges: {total_edges}\")\n",
    "\n",
    "# Calculate 5% of the edges\n",
    "five_percent_edges = int(total_edges * 0.05)\n",
    "print(f\"5% of the edges: {five_percent_edges}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now apply greedy search to the scored graph to find a circuit! We prune dead nodes, and export the circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.apply_topn(five_percent_edges , absolute=True)\n",
    "g.prune_dead_nodes()\n",
    "g.to_json('/add_sub_mul_div/graph_results_steps/graph_add_sub_1.4b_r32_epoch4_initial.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.count_included_nodes(), g.count_included_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from functools import partial\n",
    "\n",
    "# Function to calculate faithfulness and percentage of model performance\n",
    "def calculate_faithfulness(model, g, dataloader, metric_fn):\n",
    "    # Evaluate baseline (full model performance)\n",
    "    baseline_performance = evaluate_baseline(model, dataloader, metric_fn).mean().item()\n",
    "\n",
    "    # Evaluate the discovered circuit's performance\n",
    "    circuit_performance = evaluate_graph(model, g, dataloader, metric_fn).mean().item()\n",
    "\n",
    "    # Calculate the absolute difference (faithfulness)\n",
    "    faithfulness = abs(baseline_performance - circuit_performance)\n",
    "\n",
    "    # Calculate the percentage of model performance achieved by the circuit\n",
    "    percentage_performance = (1 - faithfulness / baseline_performance) * 100\n",
    "\n",
    "    print(f\"Baseline performance: {baseline_performance}\")\n",
    "    print(f\"Circuit performance: {circuit_performance}\")\n",
    "    print(f\"Faithfulness: {faithfulness}\")\n",
    "    print(f\"Percentage of model performance achieved by the circuit: {percentage_performance:.2f}%\")\n",
    "\n",
    "    return faithfulness, percentage_performance\n",
    "\n",
    "# Define the KL divergence metric (from your code)\n",
    "metric_fn = partial(kl_divergence, loss=False, mean=False)\n",
    "\n",
    "# Calculate faithfulness and percentage performance\n",
    "faithfulness, percentage_performance = calculate_faithfulness(model, g, dataloader, metric_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def exact_match_accuracy(logits, corrupted_logits, input_lengths, labels):\n",
    "\n",
    "    batch_size = logits.size(0)\n",
    "    device = logits.device\n",
    "\n",
    "    # Get the positions of the last tokens in each sequence\n",
    "    positions = input_lengths - 1  # [batch_size]\n",
    "\n",
    "    # Gather the logits at these positions\n",
    "    last_logits = logits[torch.arange(batch_size), positions, :]  # [batch_size, vocab_size]\n",
    "\n",
    "    # Get the predicted tokens\n",
    "    predicted_tokens = last_logits.argmax(dim=-1)  # [batch_size]\n",
    "\n",
    "    # Convert predicted tokens to strings\n",
    "    predicted_strings = [model.to_string(token.item()).strip() for token in predicted_tokens]\n",
    "\n",
    "    # Convert labels to strings\n",
    "    labels_strings = []\n",
    "    for i in range(batch_size):\n",
    "        lab = labels[i]\n",
    "        if isinstance(lab, torch.Tensor):\n",
    "            lab = lab.item()\n",
    "        labels_strings.append(str(lab).strip())\n",
    "\n",
    "    # Compute correctness\n",
    "    correct = []\n",
    "    for pred_str, label_str in zip(predicted_strings, labels_strings):\n",
    "        if pred_str == label_str:\n",
    "            correct.append(1.0)\n",
    "        else:\n",
    "            correct.append(0.0)\n",
    "\n",
    "    return torch.tensor(correct, device=device)\n",
    "\n",
    "# Evaluate the baseline model\n",
    "baseline_accuracy = evaluate_baseline(model, dataloader, exact_match_accuracy, quiet=True).mean().item()\n",
    "\n",
    "# Evaluate the graph model\n",
    "graph_accuracy = evaluate_graph(model, g, dataloader, exact_match_accuracy, quiet=True).mean().item()\n",
    "\n",
    "print(f\"Baseline model accuracy: {baseline_accuracy:.4f}; Graph model accuracy: {graph_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
